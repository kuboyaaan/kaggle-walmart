## What is this?
kaggle: Walmart Recruiting - Store Sales Forecasting
https://www.kaggle.com/c/walmart-recruiting-store-sales-forecasting/notebooks?sortBy=dateRun&group=profile&pageSize=20&competitionId=3816


## About File
- walmart_eda.ipynb
EDA時に作成した.ipynbファイル<br>
EDAの内容については後述する<br>

- walmart_feature_engineering.ipynb
特徴量作成・特徴量選択を行なっている.ipynbファイル<br>
後半、LightGBMのモデルの学習を回しているが、これは特徴量選択でnull importanceを確認するためであり、実際の予測モデルとは直接的な関係はないモデルの学習となる<br>

- walmart_modeling.ipynb
特徴量を作成し、学習・予測までの一連の流れを記述している.ipynbファイル

- walmart.ipynb
データ前処理・モデリングを行なった際の.ipynbファイル<br>

<!-- - walmartディレクトリ
データ前処理・モデリングの内容を.pyファイルで記述<br>
基本的にはwalmart.ipynbと処理内容は同じである<br>
`python run.py`で実行できる<br> -->


## Report
分析・予測モデル構築の一連の内容について記述する
### EDA
EDAに関して（詳しくは.ipynb内に記述）
- 1. データ形状・欠損の有無・要約統計量・uniqueな値（カテゴリ変数）など一般的なものを確認
    → 'Store'ごとの'Dept'の数の偏り、全体における'Dept'の偏りに着目
    → 目的変数の’Weekly_Sales’の分布の偏り、および負の値を持つことに着目
    → 'MarkDown1~5'の欠損に着目
- 2. 'Weekly_Sales'をヒストグラムで可視化（0より小さい値を除去し、対数変換した値を可視化）
    → 1.でも述べたが、分布の偏りが顕著
    → 0以上の値だけを取り出し、対数変換した値は左の裾は長いものの、そのままで扱うよりは正規分布に近い為、損失を小さく抑えることができそうだと感じる(評価指標も基本的にMSEの派生なので)
        → しかし、0以下の数字の表す意味が把握できないため、安易にその処理を行うのは非常に危険だと判断
- 3. 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', および'Weekly_Sales'について時系列を軸に取り可視化
    → 'Weekly_Sale'がその他の特徴量と関係があまりないのでは？（仮定）

### Reason for algorithm selection
アルゴリズム選定理由
- 1. MarkDown特徴量はプロモーションイベントとあるので、非常に重要な特徴量であることは説明されていたが、ある期間より以前には値が格納されておらず、半分程度欠損の状態
     欠損の補間する方法も考えられるが、重要な特徴量ゆえ、安易な平均や0補間などでは危険だと判断
1.の理由から特徴量が欠損の状態のまま扱える、決定木ベースで進めたいと考える
- 2. 前処理で特徴量をいくつか増やすことも考え、GBDTモデルを適用し、複数の（重なり合う可能性のある）特徴量空間から予測を行いたい
決定木１つでは、複数の特徴量空間から予測を行うには心もとない、また、ランダムフォレストのように、並列に決定木を学習するよりも、直列に決定木を学習し、弱学習機のロスを踏まえて決定木を作っていきたい
正直なところ、短時間で予測モデルを構築するには、GBDTが扱いやすいというのもある
- 3. 全体の特徴量で学習を行うと欠損の多いMarkDownの特徴量を反映できない可能性があるため、MarkDown特徴量だけ抜き出して、決定木orランダムフォレストを学習するのは１つの戦略としてありだと考える

### Ingenuity
工夫した点・特徴量エンジニアリングなど
#### CV戦略
- 1. 時系列データでは直近のデータでvalidationを行うのが、定石であるため、2or3で時系列で分割し、直近のデータをvalidationデータとして用いて学習する
- 2. しかし、MarkDown特徴量が存在するのは、最後の一年分しかないので、直近のデータをtrainデータとし、それ以前のデータをvalidationとしたモデルも作成する
    　MarkDown特徴量を用いて学習するため、こちらの重みを大きくして、最終予測とする可能性もある（validationの結果を見つつ）
- 3. 1.2.のvalidationの結果を踏まえて、iterationを見積もり、全体のデータで学習したモデルも構築する

#### 特徴量エンジニアリング
##### 特徴量作成
- 1. どの店舗のどの部門であるかの関係性は重要と考えるので、店舗と部門を連結させ、明示的に関係性を表すカテゴリ特徴を作成
- 2. 特別な休暇(SuperBowl, LaborDay, Thanksgiving, Christmas)の予測は重要であること、その前後で売り上げに影響あるだろうとの予測から、休暇の前週には-1, 当週は0, 次の週には+1を格納する特徴量を作成
- 3. 特別な休暇は休暇ごとに毛色が違う可能性があると考え、休暇の種類別にラベルを付与した特徴量を作成する
- 4. 日時情報は、sin, cosで表すことで周期性を表せるようにする

##### 特徴量選択
- 1. EDAで見たように、'Temperature', 'Fuel_Price', 'CPI', 'Unemployment'の4特徴量は目的変数との関連性があまりなさそうである
- 2. モデル構築時の気づきではあるが、1.の4カラムはGBDTモデル学習時、重要度が非常に高く、また、それ以外の特徴量が学習に上手く反映されないまま、学習が終わっていた、
- 3. null importanceを行い、特徴量の重要度を改めて確認した際に、1.の4特徴量は依然として重要度が高く、学習時に悪影響を与えていると判断
1.2.3.の利用から4特徴量を削除することに決めた
- 4. MarkDown特徴量は効果的だとあるが、MarkDown3に関してはEDAの段階ではそこまで重要そうとは考えにくい（ただ、GBDTでの重要度は高い）

#### その他
- 1. iterationにあたりをつけて、全部のデータで学習することで上手くMarkDown特徴量を反映できるようになったのではと考える（結果論）
- 2. 目的変数について、分布の偏りへの対処が上手くできていない。目的変数が0以上のデータに絞り、学習も行なってみたが、むしろ悪化した為、マイナスの値は異常値ではなく、それはそれで意味のある値であると考える
### Result and Tasks
##### Result
PublicLB: 2995.99902
PrivateLB: 3083.91267

##### Tasks
- 1. データが限られていたので、MarkDown特徴量を上手く補間することができれば、スコアの改善が期待できると考えているが、店舗や部門ごとに平均とっただけでは逆にスコアが下がってしまう
    　これらの補間をもう少し考えてみる必要があると考えた
- 2. 時間の都合上学習時のmetricを特に変更せずrmseで行なったが重み付き誤差への変更をすると改善が見込めそう（特別な休暇のある週の損失の重みが大きいので）
- 3. 目的変数の対数変換（負の値は一旦絶対値で対数をとって符号を付与することで対処など）することでモデルの学習を安定させてみるといいかもしれない
    　簡易的に負の値のレコードを除去して学習してみたが、予測精度が悪化。負の値の意味を考える必要がある
- 4. MarkDown3に関しては、単純に縮尺の影響でEDAの段階で影響が小さく見えたのかどうか厳密に見ていく必要がある
- 5. 各種パラメータに関しては、調整等あまり行なっていないため、gridSearchなどで探索してもいいかもしれない
